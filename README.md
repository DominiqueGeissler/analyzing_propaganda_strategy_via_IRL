# analyzing_propaganda_strategy_via_IRL

This is the code for the paper "Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine".

## Abstract
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda by bots and humans has remained unclear. Here, we analyze the dissemination strategy of bots and humans on Twitter using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behaviour as a Markov decision process, where the goal is to infer the underlying reward structure that guides bots and humans. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans are characterized by different incentives: bots are much more likely to retweet than humans, while humans are more likely to post after others reshared their content. To profile propaganda spreaders, we further use machine learning to test whether the different interaction dynamics set bots vs. humans aside. To the best of our knowledge, this is the first study to leverage IRL to analyze the strategy that drives propaganda dissemination by bots and humans on social media. 

## Data
You can find the author IDs and tweet IDs, which were used in this study in the folder "data". 
These can be rehydrated to get the information needed to replicate this study. 

Moreover, the folder "data" contains the results from the IRL: "df_results_bots_IRL.csv" and "df_results_users_IRL.csv".
This data is represented without the username, so that users cannot be identified.

We also included the data that is used to run the machine learning classifier: "label_IRL.csv", "data_IRL_with_rewards.csv", and "data_IRL_norewards.csv".

## Replication
### Inverse reinforcement learning
The IRL requires a csv file for each of the actions and each of the states for users and bots. 

Possible actions:
- original tweets generated by bots (resp. users)
- retweets generated by bots (resp. users)
- replies generated by bots (resp. users)
- mentions generated by bots (resp. users)

Possible states:
- retweets of content generated by bots (resp. users)
- replies to content generated by bots (resp. users)
- tweets where bots (resp. users) were mentioned

Each of these seven categories has to be grouped in a different csv file, which will be used as an input for "main_IRL.py".
Each csv file should include the following columns:

- screen_name = screen_name of the tweet author 
- text*	= tweet text
- created_at = time of tweet creation 
- id_str = tweet id
- user_statuses_count = statuses_count of the author (i.e., screen_name)
- user_followers_count*	= followers_count of the author (i.e., screen_name)
- user_friends_count* = friends_count of the author (i.e., screen_name)
- user_favourites_count* = favourites_count of the author (i.e., screen_name)
- user_retweeted = screen_name of the creator of the original tweet that has been retweeted by screen_name (if the tweet is not a retweet this field can be set to NaN)
- retweet_id_str = id_str of the original tweet that has been retweeted by screen_name (if the tweet is not a retweet this field can be set to NaN)
- retweet_count (if the tweet is not a retweet this field can be set to NaN)
- retweet_favorite_count = favorite_count of the retweet (if the tweet is not a retweet this field can be set to NaN)
- retweet_reply_count = reply_count of the retweet (if the tweet is not a retweet this field can be set to NaN)
- user_retweeted_statuses_count = statuses_count of the retweeted user (i.e., user_retweeted)
- user_retweeted_followers* = followers_count of the retweeted user (i.e., user_retweeted)
- user_retweeted_friends* = friends_count of the retweeted user (i.e., user_retweeted)
- user_retweeted_favourites_count* = favourites_count of the retweeted user (i.e., user_retweeted)
- mention = screen_name of the mentioned user (this field can be set to NaN if the tweet does not have mentions)
- in_reply_to_screen_name = screen_name of the user that received a reply (if the tweet is not a reply this field can be set to NaN)
- in_reply_to_status_id_str = id_str of the tweet that received a reply (if the tweet is not a reply this field can be set to NaN)	


For more information, please refer to the implementation by Luca Luceri, 2020 (https://github.com/luceriluc/Detecting-Troll-Behavior-via-Inverse-Reinforcement-Learning).

*Column not needed to be filled for the current implementation. 
You can fill the column with a random value or leave it empty, if you do not have this data.

### Machine learning classification
You can replicate the machine learning classification by running "ML_models.py". 
We included the data (labels, features with and without rewards) in the "data" directory.

### Plots
You can replicate the plots in the study by running "analysis_rewards.py" and "analysis_weights.py". 
These use the output from the IRL. 
We included the data in the "data" directory.