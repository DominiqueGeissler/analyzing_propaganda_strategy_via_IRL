# analyzing_propaganda_strategy_via_IRL

This is the code for the paper "Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine".

## Abstract
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.

## Data
You can find the author IDs and tweet IDs, which were used in this study in the folder "data". 
These can be rehydrated to get the information needed to replicate this study. 

Moreover, the folder "data" contains the results from the IRL: "df_results_all_IRL.csv", "df_results_bots_IRL.csv" and "df_results_users_IRL.csv".
This data is represented without the username, so that users cannot be identified.

We also included the replies and mentions and their corresponding stance label: "replies_label.csv".
In addition, we added the top ten words for support and opposition as extracted be the log odds ratio.

## Replication
### Inverse reinforcement learning
The IRL requires a csv file for each of the actions and each of the states for users and bots. 

Possible actions:
- original tweets generated by bots (resp. users)
- retweets generated by bots (resp. users)
- replies generated by bots (resp. users)
- mentions generated by bots (resp. users)

Possible states:
- retweets of content generated by bots (resp. users)
- supporting replies to content generated by bots (resp. users)
- opposing replies to content generated by bots (resp. users)
- tweets where bots (resp. users) were mentioned

Each of these seven categories has to be grouped in a different csv file, which will be used as an input for "main_IRL.py".
Each csv file should include the following columns:

- screen_name = screen_name of the tweet author 
- text*	= tweet text
- created_at = time of tweet creation 
- id_str = tweet id
- user_statuses_count = statuses_count of the author (i.e., screen_name)
- user_followers_count*	= followers_count of the author (i.e., screen_name)
- user_friends_count* = friends_count of the author (i.e., screen_name)
- user_favourites_count* = favourites_count of the author (i.e., screen_name)
- user_retweeted = screen_name of the creator of the original tweet that has been retweeted by screen_name (if the tweet is not a retweet this field can be set to NaN)
- retweet_id_str = id_str of the original tweet that has been retweeted by screen_name (if the tweet is not a retweet this field can be set to NaN)
- retweet_count (if the tweet is not a retweet this field can be set to NaN)
- retweet_favorite_count = favorite_count of the retweet (if the tweet is not a retweet this field can be set to NaN)
- retweet_reply_count = reply_count of the retweet (if the tweet is not a retweet this field can be set to NaN)
- user_retweeted_statuses_count = statuses_count of the retweeted user (i.e., user_retweeted)
- user_retweeted_followers* = followers_count of the retweeted user (i.e., user_retweeted)
- user_retweeted_friends* = friends_count of the retweeted user (i.e., user_retweeted)
- user_retweeted_favourites_count* = favourites_count of the retweeted user (i.e., user_retweeted)
- mention = screen_name of the mentioned user (this field can be set to NaN if the tweet does not have mentions)
- in_reply_to_screen_name = screen_name of the user that received a reply (if the tweet is not a reply this field can be set to NaN)
- in_reply_to_status_id_str = id_str of the tweet that received a reply (if the tweet is not a reply this field can be set to NaN)	


For more information, please refer to the implementation by Luca Luceri, 2020 (https://github.com/luceriluc/Detecting-Troll-Behavior-via-Inverse-Reinforcement-Learning).

*Column not needed to be filled for the current implementation. 
You can fill the column with a random value or leave it empty, if you do not have this data.

### Machine learning classification
You can replicate the stance detection by running "bert_finetuning.py". 
We included the top ten stance tokens according to log odds ratio in the "data" directory.

### Plots
You can replicate the plots in the study by running "analysis_rewards.py" and "analysis_weights.py" in directory "figures". 
These use the output from the IRL. 
We included the data in the "data" directory.